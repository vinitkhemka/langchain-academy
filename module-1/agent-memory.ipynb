{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cd1c3e",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/agent-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239417-lesson-7-agent-with-memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c451ffd-a18b-4412-85fa-85186824dd03",
   "metadata": {},
   "source": [
    "# Agent memory\n",
    "\n",
    "## Review\n",
    "\n",
    "Previously, we built an agent that can:\n",
    "\n",
    "* `act` - let the model call specific tools \n",
    "* `observe` - pass the tool output back to the model \n",
    "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
    "\n",
    "![Screenshot 2024-08-21 at 12.45.32 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab7453080e6802cd1703_agent-memory1.png)\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going extend our agent by introducing memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4b45b-cbaa-41b1-b3ed-f6b0645be3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langgraph langgraph-prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0cfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff247-a2aa-4f7a-8be1-73dfebfecc63",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ef2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f123b-db5d-4816-a6a3-2e4247611512",
   "metadata": {},
   "source": [
    "This follows what we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46647bbe-def5-4ea7-a315-1de8d97c8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI as ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "@tool\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "def greeting(name:str) -> str:\n",
    "    \"\"\"Greet a person\"\"\"\n",
    "    return (\"Howya doing!!\", name)\n",
    "\n",
    "def general_llm(text:str) ->str:\n",
    "    \"\"\"general purpose llm\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gemini-2.0-flash-lite\")\n",
    "    response= llm.invoke(text)\n",
    "    print(\"resposne:\",response)\n",
    "    return response\n",
    "\n",
    "tools = [add, multiply, divide,greeting, general_llm]\n",
    "llm = ChatOpenAI(model=\"gemini-2.0-flash-lite\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9092b40-20c4-4872-b0ed-be1b53a15ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant Which selects the best course of action and execute it through\" \\\n",
    "\"various tools\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   print(\"here\",state)\n",
    "   x={\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "   print(\"state modifed\",x)\n",
    "   return x\n",
    "\n",
    "def greet_assistant(state:MessagesState):\n",
    "   print('greet',state)\n",
    "   return {\"messages\":[llm_greet.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771123a3-91ac-4076-92c0-93bcd69cf048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "# builder.add_node(\"greetings\",greet_assistant)\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "# builder.add_edge(\"assistant\",\"greetings\")\n",
    "\n",
    "builder.add_edge( \"tools\",\"assistant\")\n",
    "# builder.add_edge(\"greetings\",END)\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "try:\n",
    "    display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830b7ae-3673-4cc6-8627-4740b7b8b217",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Let's run our agent, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596a71a0-1337-44d4-971d-f80c367bd868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here {'messages': [HumanMessage(content='what a transformer architecture', additional_kwargs={}, response_metadata={}, id='730106c9-1596-4aaa-8fa1-994993641d1e')]}\n",
      "state modifed {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'general_llm', 'arguments': '{\"text\": \"what a transformer architecture\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--ff17cc36-f06d-42cd-95c3-156f296388b6-0', tool_calls=[{'name': 'general_llm', 'args': {'text': 'what a transformer architecture'}, 'id': '4152152c-93b3-4527-aa82-cdaaf471bb63', 'type': 'tool_call'}], usage_metadata={'input_tokens': 129, 'output_tokens': 9, 'total_tokens': 138, 'input_token_details': {'cache_read': 0}})]}\n",
      "resposne: content='A transformer architecture is a deep learning model that has revolutionized the field of natural language processing (NLP) and is also finding applications in computer vision and other areas.  It\\'s primarily known for its ability to process sequential data, like text, in a highly parallelizable and efficient manner.  Here\\'s a breakdown of its key components and concepts:\\n\\n**Core Idea:  Attention Mechanism**\\n\\nThe central innovation of the transformer is the **attention mechanism**.  Instead of relying on recurrent connections (like in RNNs or LSTMs) to process sequences, transformers use attention to directly model relationships between all parts of the input sequence.  This allows the model to understand which parts of the input are most relevant to each other when making predictions.\\n\\n**Key Components:**\\n\\n1.  **Input Embedding:**\\n    *   The input, whether it\\'s text (words, subwords), image patches, or other sequential data, is first converted into numerical representations called embeddings. Each word, subword, or patch is assigned a vector.\\n    *   A positional encoding is added to the embeddings.  This provides information about the position of each element in the sequence, as the attention mechanism itself doesn\\'t inherently understand order.  Common positional encodings use sine and cosine functions of different frequencies.\\n\\n2.  **Encoder:**\\n    *   The encoder processes the input embeddings to create a contextualized representation of the input sequence.  It\\'s made up of multiple identical layers stacked on top of each other.\\n    *   **Each Encoder Layer Consists of:**\\n        *   **Multi-Head Self-Attention:** This is the heart of the transformer.  It allows the model to attend to different parts of the input sequence and learn relationships between them.  Here\\'s how it works:\\n            *   **Query, Key, Value:**  The input embeddings are transformed into three different matrices: Query (Q), Key (K), and Value (V).  These are typically learned through linear transformations.\\n            *   **Scaled Dot-Product Attention (Single Head):**\\n                *   Calculate the dot products of the Query matrix with the Key matrix (Q * K^T). This measures the similarity between each word (Query) and all other words (Keys).\\n                *   Scale the dot products by the square root of the dimension of the key vectors (to prevent large values that can saturate the softmax).\\n                *   Apply a softmax function to the scaled dot products.  This converts the scores into probabilities, representing the attention weights for each word in relation to another.\\n                *   Multiply the attention weights by the Value matrix (V).  This combines the values based on the attention weights, effectively giving more weight to the values of the most relevant words.\\n            *   **Multi-Head Attention:**  The process of calculating attention is performed multiple times in parallel (using different learned weight matrices for Q, K, and V).  The outputs from each \"head\" are then concatenated and linearly transformed to produce the final output of the self-attention layer.  This allows the model to capture different aspects of the relationships between words.\\n        *   **Add & Norm (Residual Connection and Layer Normalization):**  A residual connection adds the original input of the self-attention layer to its output (this helps with gradient flow during training).  Layer normalization is then applied to stabilize the training process.\\n        *   **Feed Forward Network:**  A feed-forward neural network (typically a two-layer network with a ReLU activation) is applied to each position in the sequence independently.  This adds non-linearity and allows the model to learn more complex relationships.\\n        *   **Add & Norm (again):** Another residual connection and layer normalization is applied to the output of the feed-forward network.\\n\\n3.  **Decoder:**\\n    *   The decoder\\'s role is to generate the output sequence based on the encoded representation from the encoder and the previously generated output tokens (in the case of tasks like machine translation).  Similar to the encoder, it also consists of multiple layers.\\n    *   **Each Decoder Layer Consists of:**\\n        *   **Masked Multi-Head Self-Attention:**  This is similar to the encoder\\'s self-attention, but it\\'s *masked*.  The masking prevents the decoder from attending to future tokens in the output sequence during training.  This is crucial for autoregressive generation (generating one token at a time).\\n        *   **Encoder-Decoder Attention:**  This layer attends to the output of the encoder.  The Query matrix is derived from the output of the masked self-attention layer, and the Key and Value matrices are derived from the encoder\\'s output.  This allows the decoder to focus on the relevant parts of the input sequence while generating the output.\\n        *   **Add & Norm:** Residual connections and layer normalization.\\n        *   **Feed Forward Network:**  Another feed-forward neural network, similar to the encoder.\\n        *   **Add & Norm:** Residual connections and layer normalization.\\n\\n4.  **Output Linear Layer and Softmax:**\\n    *   The output of the final decoder layer is passed through a linear layer to project the hidden states into the vocabulary space (for tasks like language modeling or translation).\\n    *   A softmax function is applied to the output of the linear layer to produce a probability distribution over the vocabulary, representing the model\\'s prediction for the next token.\\n\\n**How it Works (Simplified Example: Machine Translation)**\\n\\n1.  **Encoding (Source Language):**  The encoder processes the input sentence in the source language (e.g., English) and creates a contextualized representation of the sentence.\\n2.  **Decoding (Target Language):** The decoder receives the encoded representation from the encoder.  It starts with a special \"start-of-sequence\" token.\\n3.  **Autoregressive Generation:**  The decoder generates the output sentence in the target language (e.g., French) one word at a time.  In each step:\\n    *   The decoder attends to the encoded source sentence (encoder-decoder attention).\\n    *   The decoder attends to the previously generated words (masked self-attention).\\n    *   The decoder predicts the next word in the target language (softmax output).\\n    *   The predicted word is appended to the output sequence, and the process repeats until an \"end-of-sequence\" token is generated.\\n\\n**Advantages of Transformers:**\\n\\n*   **Parallelism:**  Unlike RNNs, transformers can process the entire input sequence in parallel, making them much faster to train and use, especially on GPUs.\\n*   **Long-Range Dependencies:**  The attention mechanism allows transformers to effectively model relationships between words that are far apart in the sequence, which is a challenge for RNNs.\\n*   **Contextualized Word Embeddings:**  Transformers learn context-aware representations of words, meaning that the meaning of a word changes depending on the surrounding words.\\n*   **State-of-the-Art Performance:**  Transformers have achieved state-of-the-art results in many NLP tasks, including machine translation, text summarization, question answering, and more.\\n*   **Transfer Learning:**  Pre-trained transformer models (like BERT, GPT, and others) can be fine-tuned for specific tasks, significantly reducing the amount of training data and time required.\\n\\n**Disadvantages and Considerations:**\\n\\n*   **Computational Cost:**  Transformers can be computationally expensive, especially for very long sequences.\\n*   **Data Requirements:**  Training transformer models from scratch often requires a large amount of data.\\n*   **Overfitting:**  Transformers can be prone to overfitting, especially on small datasets.  Regularization techniques (dropout, weight decay) are often used.\\n*   **Interpretability:**  While attention weights provide some insight, understanding *why* a transformer makes a specific prediction can be challenging.\\n\\n**Key Transformer Architectures and Models:**\\n\\n*   **BERT (Bidirectional Encoder Representations from Transformers):**  A powerful encoder-only model used for understanding the meaning of text.\\n*   **GPT (Generative Pre-trained Transformer):**  A decoder-only model used for text generation.  GPT models are often used for tasks like text completion, creative writing, and dialogue generation.\\n*   **T5 (Text-to-Text Transfer Transformer):**  A unified framework where all NLP tasks are framed as text-to-text problems (e.g., translation becomes \"translate English to French: ...\").\\n*   **ViT (Vision Transformer):** Applies the transformer architecture to image processing, treating images as sequences of patches.\\n*   And many others...\\n\\n**In Summary:**\\n\\nThe transformer architecture is a powerful and versatile deep learning model based on the attention mechanism.  It has revolutionized NLP and is increasingly being applied to other domains.  Its ability to handle sequential data effectively, its parallelizability, and its capacity for transfer learning make it a dominant force in the field.  Understanding the core components of the encoder, decoder, and the attention mechanism is key to grasping how transformers work.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []} id='run--a6f44071-9fe1-499c-813d-7579e6ddf073-0' usage_metadata={'input_tokens': 4, 'output_tokens': 1915, 'total_tokens': 1919, 'input_token_details': {'cache_read': 0}}\n",
      "here {'messages': [HumanMessage(content='what a transformer architecture', additional_kwargs={}, response_metadata={}, id='730106c9-1596-4aaa-8fa1-994993641d1e'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'general_llm', 'arguments': '{\"text\": \"what a transformer architecture\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--ff17cc36-f06d-42cd-95c3-156f296388b6-0', tool_calls=[{'name': 'general_llm', 'args': {'text': 'what a transformer architecture'}, 'id': '4152152c-93b3-4527-aa82-cdaaf471bb63', 'type': 'tool_call'}], usage_metadata={'input_tokens': 129, 'output_tokens': 9, 'total_tokens': 138, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='content=\\'A transformer architecture is a deep learning model that has revolutionized the field of natural language processing (NLP) and is also finding applications in computer vision and other areas.  It\\\\\\'s primarily known for its ability to process sequential data, like text, in a highly parallelizable and efficient manner.  Here\\\\\\'s a breakdown of its key components and concepts:\\\\n\\\\n**Core Idea:  Attention Mechanism**\\\\n\\\\nThe central innovation of the transformer is the **attention mechanism**.  Instead of relying on recurrent connections (like in RNNs or LSTMs) to process sequences, transformers use attention to directly model relationships between all parts of the input sequence.  This allows the model to understand which parts of the input are most relevant to each other when making predictions.\\\\n\\\\n**Key Components:**\\\\n\\\\n1.  **Input Embedding:**\\\\n    *   The input, whether it\\\\\\'s text (words, subwords), image patches, or other sequential data, is first converted into numerical representations called embeddings. Each word, subword, or patch is assigned a vector.\\\\n    *   A positional encoding is added to the embeddings.  This provides information about the position of each element in the sequence, as the attention mechanism itself doesn\\\\\\'t inherently understand order.  Common positional encodings use sine and cosine functions of different frequencies.\\\\n\\\\n2.  **Encoder:**\\\\n    *   The encoder processes the input embeddings to create a contextualized representation of the input sequence.  It\\\\\\'s made up of multiple identical layers stacked on top of each other.\\\\n    *   **Each Encoder Layer Consists of:**\\\\n        *   **Multi-Head Self-Attention:** This is the heart of the transformer.  It allows the model to attend to different parts of the input sequence and learn relationships between them.  Here\\\\\\'s how it works:\\\\n            *   **Query, Key, Value:**  The input embeddings are transformed into three different matrices: Query (Q), Key (K), and Value (V).  These are typically learned through linear transformations.\\\\n            *   **Scaled Dot-Product Attention (Single Head):**\\\\n                *   Calculate the dot products of the Query matrix with the Key matrix (Q * K^T). This measures the similarity between each word (Query) and all other words (Keys).\\\\n                *   Scale the dot products by the square root of the dimension of the key vectors (to prevent large values that can saturate the softmax).\\\\n                *   Apply a softmax function to the scaled dot products.  This converts the scores into probabilities, representing the attention weights for each word in relation to another.\\\\n                *   Multiply the attention weights by the Value matrix (V).  This combines the values based on the attention weights, effectively giving more weight to the values of the most relevant words.\\\\n            *   **Multi-Head Attention:**  The process of calculating attention is performed multiple times in parallel (using different learned weight matrices for Q, K, and V).  The outputs from each \"head\" are then concatenated and linearly transformed to produce the final output of the self-attention layer.  This allows the model to capture different aspects of the relationships between words.\\\\n        *   **Add & Norm (Residual Connection and Layer Normalization):**  A residual connection adds the original input of the self-attention layer to its output (this helps with gradient flow during training).  Layer normalization is then applied to stabilize the training process.\\\\n        *   **Feed Forward Network:**  A feed-forward neural network (typically a two-layer network with a ReLU activation) is applied to each position in the sequence independently.  This adds non-linearity and allows the model to learn more complex relationships.\\\\n        *   **Add & Norm (again):** Another residual connection and layer normalization is applied to the output of the feed-forward network.\\\\n\\\\n3.  **Decoder:**\\\\n    *   The decoder\\\\\\'s role is to generate the output sequence based on the encoded representation from the encoder and the previously generated output tokens (in the case of tasks like machine translation).  Similar to the encoder, it also consists of multiple layers.\\\\n    *   **Each Decoder Layer Consists of:**\\\\n        *   **Masked Multi-Head Self-Attention:**  This is similar to the encoder\\\\\\'s self-attention, but it\\\\\\'s *masked*.  The masking prevents the decoder from attending to future tokens in the output sequence during training.  This is crucial for autoregressive generation (generating one token at a time).\\\\n        *   **Encoder-Decoder Attention:**  This layer attends to the output of the encoder.  The Query matrix is derived from the output of the masked self-attention layer, and the Key and Value matrices are derived from the encoder\\\\\\'s output.  This allows the decoder to focus on the relevant parts of the input sequence while generating the output.\\\\n        *   **Add & Norm:** Residual connections and layer normalization.\\\\n        *   **Feed Forward Network:**  Another feed-forward neural network, similar to the encoder.\\\\n        *   **Add & Norm:** Residual connections and layer normalization.\\\\n\\\\n4.  **Output Linear Layer and Softmax:**\\\\n    *   The output of the final decoder layer is passed through a linear layer to project the hidden states into the vocabulary space (for tasks like language modeling or translation).\\\\n    *   A softmax function is applied to the output of the linear layer to produce a probability distribution over the vocabulary, representing the model\\\\\\'s prediction for the next token.\\\\n\\\\n**How it Works (Simplified Example: Machine Translation)**\\\\n\\\\n1.  **Encoding (Source Language):**  The encoder processes the input sentence in the source language (e.g., English) and creates a contextualized representation of the sentence.\\\\n2.  **Decoding (Target Language):** The decoder receives the encoded representation from the encoder.  It starts with a special \"start-of-sequence\" token.\\\\n3.  **Autoregressive Generation:**  The decoder generates the output sentence in the target language (e.g., French) one word at a time.  In each step:\\\\n    *   The decoder attends to the encoded source sentence (encoder-decoder attention).\\\\n    *   The decoder attends to the previously generated words (masked self-attention).\\\\n    *   The decoder predicts the next word in the target language (softmax output).\\\\n    *   The predicted word is appended to the output sequence, and the process repeats until an \"end-of-sequence\" token is generated.\\\\n\\\\n**Advantages of Transformers:**\\\\n\\\\n*   **Parallelism:**  Unlike RNNs, transformers can process the entire input sequence in parallel, making them much faster to train and use, especially on GPUs.\\\\n*   **Long-Range Dependencies:**  The attention mechanism allows transformers to effectively model relationships between words that are far apart in the sequence, which is a challenge for RNNs.\\\\n*   **Contextualized Word Embeddings:**  Transformers learn context-aware representations of words, meaning that the meaning of a word changes depending on the surrounding words.\\\\n*   **State-of-the-Art Performance:**  Transformers have achieved state-of-the-art results in many NLP tasks, including machine translation, text summarization, question answering, and more.\\\\n*   **Transfer Learning:**  Pre-trained transformer models (like BERT, GPT, and others) can be fine-tuned for specific tasks, significantly reducing the amount of training data and time required.\\\\n\\\\n**Disadvantages and Considerations:**\\\\n\\\\n*   **Computational Cost:**  Transformers can be computationally expensive, especially for very long sequences.\\\\n*   **Data Requirements:**  Training transformer models from scratch often requires a large amount of data.\\\\n*   **Overfitting:**  Transformers can be prone to overfitting, especially on small datasets.  Regularization techniques (dropout, weight decay) are often used.\\\\n*   **Interpretability:**  While attention weights provide some insight, understanding *why* a transformer makes a specific prediction can be challenging.\\\\n\\\\n**Key Transformer Architectures and Models:**\\\\n\\\\n*   **BERT (Bidirectional Encoder Representations from Transformers):**  A powerful encoder-only model used for understanding the meaning of text.\\\\n*   **GPT (Generative Pre-trained Transformer):**  A decoder-only model used for text generation.  GPT models are often used for tasks like text completion, creative writing, and dialogue generation.\\\\n*   **T5 (Text-to-Text Transfer Transformer):**  A unified framework where all NLP tasks are framed as text-to-text problems (e.g., translation becomes \"translate English to French: ...\").\\\\n*   **ViT (Vision Transformer):** Applies the transformer architecture to image processing, treating images as sequences of patches.\\\\n*   And many others...\\\\n\\\\n**In Summary:**\\\\n\\\\nThe transformer architecture is a powerful and versatile deep learning model based on the attention mechanism.  It has revolutionized NLP and is increasingly being applied to other domains.  Its ability to handle sequential data effectively, its parallelizability, and its capacity for transfer learning make it a dominant force in the field.  Understanding the core components of the encoder, decoder, and the attention mechanism is key to grasping how transformers work.\\' additional_kwargs={} response_metadata={\\'prompt_feedback\\': {\\'block_reason\\': 0, \\'safety_ratings\\': []}, \\'finish_reason\\': \\'STOP\\', \\'model_name\\': \\'gemini-2.0-flash-lite\\', \\'safety_ratings\\': []} id=\\'run--a6f44071-9fe1-499c-813d-7579e6ddf073-0\\' usage_metadata={\\'input_tokens\\': 4, \\'output_tokens\\': 1915, \\'total_tokens\\': 1919, \\'input_token_details\\': {\\'cache_read\\': 0}}', name='general_llm', id='e91e53e1-3f2c-406e-8249-3a0941592002', tool_call_id='4152152c-93b3-4527-aa82-cdaaf471bb63')]}\n",
      "state modifed {'messages': [AIMessage(content=\"A transformer architecture is a deep learning model that has revolutionized the field of natural language processing (NLP). It's primarily known for its ability to process sequential data, like text, in a highly parallelizable and efficient manner. The core idea of the transformer is the attention mechanism. Instead of relying on recurrent connections, transformers use attention to directly model relationships between all parts of the input sequence.\\nKey components include:\\n*   Input Embedding\\n*   Encoder\\n*   Decoder\\n*   Output Linear Layer and Softmax\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--e848bf7e-9695-4551-ab43-55004eaee0ee-0', usage_metadata={'input_tokens': 2254, 'output_tokens': 108, 'total_tokens': 2362, 'input_token_details': {'cache_read': 0}})]}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what a transformer architecture\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  general_llm (4152152c-93b3-4527-aa82-cdaaf471bb63)\n",
      " Call ID: 4152152c-93b3-4527-aa82-cdaaf471bb63\n",
      "  Args:\n",
      "    text: what a transformer architecture\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: general_llm\n",
      "\n",
      "content='A transformer architecture is a deep learning model that has revolutionized the field of natural language processing (NLP) and is also finding applications in computer vision and other areas.  It\\'s primarily known for its ability to process sequential data, like text, in a highly parallelizable and efficient manner.  Here\\'s a breakdown of its key components and concepts:\\n\\n**Core Idea:  Attention Mechanism**\\n\\nThe central innovation of the transformer is the **attention mechanism**.  Instead of relying on recurrent connections (like in RNNs or LSTMs) to process sequences, transformers use attention to directly model relationships between all parts of the input sequence.  This allows the model to understand which parts of the input are most relevant to each other when making predictions.\\n\\n**Key Components:**\\n\\n1.  **Input Embedding:**\\n    *   The input, whether it\\'s text (words, subwords), image patches, or other sequential data, is first converted into numerical representations called embeddings. Each word, subword, or patch is assigned a vector.\\n    *   A positional encoding is added to the embeddings.  This provides information about the position of each element in the sequence, as the attention mechanism itself doesn\\'t inherently understand order.  Common positional encodings use sine and cosine functions of different frequencies.\\n\\n2.  **Encoder:**\\n    *   The encoder processes the input embeddings to create a contextualized representation of the input sequence.  It\\'s made up of multiple identical layers stacked on top of each other.\\n    *   **Each Encoder Layer Consists of:**\\n        *   **Multi-Head Self-Attention:** This is the heart of the transformer.  It allows the model to attend to different parts of the input sequence and learn relationships between them.  Here\\'s how it works:\\n            *   **Query, Key, Value:**  The input embeddings are transformed into three different matrices: Query (Q), Key (K), and Value (V).  These are typically learned through linear transformations.\\n            *   **Scaled Dot-Product Attention (Single Head):**\\n                *   Calculate the dot products of the Query matrix with the Key matrix (Q * K^T). This measures the similarity between each word (Query) and all other words (Keys).\\n                *   Scale the dot products by the square root of the dimension of the key vectors (to prevent large values that can saturate the softmax).\\n                *   Apply a softmax function to the scaled dot products.  This converts the scores into probabilities, representing the attention weights for each word in relation to another.\\n                *   Multiply the attention weights by the Value matrix (V).  This combines the values based on the attention weights, effectively giving more weight to the values of the most relevant words.\\n            *   **Multi-Head Attention:**  The process of calculating attention is performed multiple times in parallel (using different learned weight matrices for Q, K, and V).  The outputs from each \"head\" are then concatenated and linearly transformed to produce the final output of the self-attention layer.  This allows the model to capture different aspects of the relationships between words.\\n        *   **Add & Norm (Residual Connection and Layer Normalization):**  A residual connection adds the original input of the self-attention layer to its output (this helps with gradient flow during training).  Layer normalization is then applied to stabilize the training process.\\n        *   **Feed Forward Network:**  A feed-forward neural network (typically a two-layer network with a ReLU activation) is applied to each position in the sequence independently.  This adds non-linearity and allows the model to learn more complex relationships.\\n        *   **Add & Norm (again):** Another residual connection and layer normalization is applied to the output of the feed-forward network.\\n\\n3.  **Decoder:**\\n    *   The decoder\\'s role is to generate the output sequence based on the encoded representation from the encoder and the previously generated output tokens (in the case of tasks like machine translation).  Similar to the encoder, it also consists of multiple layers.\\n    *   **Each Decoder Layer Consists of:**\\n        *   **Masked Multi-Head Self-Attention:**  This is similar to the encoder\\'s self-attention, but it\\'s *masked*.  The masking prevents the decoder from attending to future tokens in the output sequence during training.  This is crucial for autoregressive generation (generating one token at a time).\\n        *   **Encoder-Decoder Attention:**  This layer attends to the output of the encoder.  The Query matrix is derived from the output of the masked self-attention layer, and the Key and Value matrices are derived from the encoder\\'s output.  This allows the decoder to focus on the relevant parts of the input sequence while generating the output.\\n        *   **Add & Norm:** Residual connections and layer normalization.\\n        *   **Feed Forward Network:**  Another feed-forward neural network, similar to the encoder.\\n        *   **Add & Norm:** Residual connections and layer normalization.\\n\\n4.  **Output Linear Layer and Softmax:**\\n    *   The output of the final decoder layer is passed through a linear layer to project the hidden states into the vocabulary space (for tasks like language modeling or translation).\\n    *   A softmax function is applied to the output of the linear layer to produce a probability distribution over the vocabulary, representing the model\\'s prediction for the next token.\\n\\n**How it Works (Simplified Example: Machine Translation)**\\n\\n1.  **Encoding (Source Language):**  The encoder processes the input sentence in the source language (e.g., English) and creates a contextualized representation of the sentence.\\n2.  **Decoding (Target Language):** The decoder receives the encoded representation from the encoder.  It starts with a special \"start-of-sequence\" token.\\n3.  **Autoregressive Generation:**  The decoder generates the output sentence in the target language (e.g., French) one word at a time.  In each step:\\n    *   The decoder attends to the encoded source sentence (encoder-decoder attention).\\n    *   The decoder attends to the previously generated words (masked self-attention).\\n    *   The decoder predicts the next word in the target language (softmax output).\\n    *   The predicted word is appended to the output sequence, and the process repeats until an \"end-of-sequence\" token is generated.\\n\\n**Advantages of Transformers:**\\n\\n*   **Parallelism:**  Unlike RNNs, transformers can process the entire input sequence in parallel, making them much faster to train and use, especially on GPUs.\\n*   **Long-Range Dependencies:**  The attention mechanism allows transformers to effectively model relationships between words that are far apart in the sequence, which is a challenge for RNNs.\\n*   **Contextualized Word Embeddings:**  Transformers learn context-aware representations of words, meaning that the meaning of a word changes depending on the surrounding words.\\n*   **State-of-the-Art Performance:**  Transformers have achieved state-of-the-art results in many NLP tasks, including machine translation, text summarization, question answering, and more.\\n*   **Transfer Learning:**  Pre-trained transformer models (like BERT, GPT, and others) can be fine-tuned for specific tasks, significantly reducing the amount of training data and time required.\\n\\n**Disadvantages and Considerations:**\\n\\n*   **Computational Cost:**  Transformers can be computationally expensive, especially for very long sequences.\\n*   **Data Requirements:**  Training transformer models from scratch often requires a large amount of data.\\n*   **Overfitting:**  Transformers can be prone to overfitting, especially on small datasets.  Regularization techniques (dropout, weight decay) are often used.\\n*   **Interpretability:**  While attention weights provide some insight, understanding *why* a transformer makes a specific prediction can be challenging.\\n\\n**Key Transformer Architectures and Models:**\\n\\n*   **BERT (Bidirectional Encoder Representations from Transformers):**  A powerful encoder-only model used for understanding the meaning of text.\\n*   **GPT (Generative Pre-trained Transformer):**  A decoder-only model used for text generation.  GPT models are often used for tasks like text completion, creative writing, and dialogue generation.\\n*   **T5 (Text-to-Text Transfer Transformer):**  A unified framework where all NLP tasks are framed as text-to-text problems (e.g., translation becomes \"translate English to French: ...\").\\n*   **ViT (Vision Transformer):** Applies the transformer architecture to image processing, treating images as sequences of patches.\\n*   And many others...\\n\\n**In Summary:**\\n\\nThe transformer architecture is a powerful and versatile deep learning model based on the attention mechanism.  It has revolutionized NLP and is increasingly being applied to other domains.  Its ability to handle sequential data effectively, its parallelizability, and its capacity for transfer learning make it a dominant force in the field.  Understanding the core components of the encoder, decoder, and the attention mechanism is key to grasping how transformers work.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []} id='run--a6f44071-9fe1-499c-813d-7579e6ddf073-0' usage_metadata={'input_tokens': 4, 'output_tokens': 1915, 'total_tokens': 1919, 'input_token_details': {'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "A transformer architecture is a deep learning model that has revolutionized the field of natural language processing (NLP). It's primarily known for its ability to process sequential data, like text, in a highly parallelizable and efficient manner. The core idea of the transformer is the attention mechanism. Instead of relying on recurrent connections, transformers use attention to directly model relationships between all parts of the input sequence.\n",
      "Key components include:\n",
      "*   Input Embedding\n",
      "*   Encoder\n",
      "*   Decoder\n",
      "*   Output Linear Layer and Softmax\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"what a transformer architecture\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8128c-f4a5-4dee-b20b-3245bd33f6b3",
   "metadata": {},
   "source": [
    "Now, let's multiply by 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b41cc1d7-e6de-4d86-8958-8cf7446f4c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here {'messages': [HumanMessage(content='Multiply that by 2.', additional_kwargs={}, response_metadata={}, id='919daddb-93b7-4e61-9d21-99f59eeef64f')]}\n",
      "messages\n",
      "state modifed {'messages': [AIMessage(content='I need two numbers to multiply. Could you please provide them?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--b8b1b5a2-2a00-4ffe-92ab-68b97dca18b0-0', usage_metadata={'input_tokens': 119, 'output_tokens': 14, 'total_tokens': 133, 'input_token_details': {'cache_read': 0}})]}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply that by 2.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I need two numbers to multiply. Could you please provide them?\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e65f3c-e1dc-4a62-b8ab-02b33a6ff268",
   "metadata": {},
   "source": [
    "We don't retain memory of 7 from our initial chat!\n",
    "\n",
    "This is because [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
    "\n",
    "Of course, this limits our ability to have multi-turn conversations with interruptions. \n",
    "\n",
    "We can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this! \n",
    "\n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
    "\n",
    "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update. \n",
    "\n",
    "One of the easiest checkpointers to use is the `MemorySaver`, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is simply compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d327c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "637fcd79-3896-42e4-9131-e03b123a0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "react_graph_memory = builder.compile(checkpointer=memory, cache=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff8fc3bf-3999-47cb-af34-06b2b94d7192",
   "metadata": {},
   "source": [
    "When we use memory, we need to specify a `thread_id`.\n",
    "\n",
    "This `thread_id` will store our collection of graph states.\n",
    "\n",
    "Here is a cartoon:\n",
    "\n",
    "* The checkpointer write the state at every step of the graph\n",
    "* These checkpoints are saved in a thread \n",
    "* We can access that thread in the future using the `thread_id`\n",
    "\n",
    "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f722a1d6-e73c-4023-86ed-8b07d392278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here {'messages': [HumanMessage(content='Add 3 and 4.', additional_kwargs={}, response_metadata={}, id='74547d33-921c-46ec-9802-5db5ec4df1b0')]}\n",
      "state modifed {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"b\": 4.0, \"a\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--e86ca31b-01ac-4136-8ece-1005b294ca49-0', tool_calls=[{'name': 'add', 'args': {'b': 4.0, 'a': 3.0}, 'id': 'b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 5, 'total_tokens': 137, 'input_token_details': {'cache_read': 0}})]}\n",
      "here {'messages': [HumanMessage(content='Add 3 and 4.', additional_kwargs={}, response_metadata={}, id='74547d33-921c-46ec-9802-5db5ec4df1b0'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"b\": 4.0, \"a\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--e86ca31b-01ac-4136-8ece-1005b294ca49-0', tool_calls=[{'name': 'add', 'args': {'b': 4.0, 'a': 3.0}, 'id': 'b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 5, 'total_tokens': 137, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='7', name='add', id='eaf6016b-d63d-4b22-b402-e65737fe7679', tool_call_id='b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7')]}\n",
      "state modifed {'messages': [AIMessage(content='The result is 7.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--d897f114-6051-494a-9703-6b533630e2c2-0', usage_metadata={'input_tokens': 140, 'output_tokens': 7, 'total_tokens': 147, 'input_token_details': {'cache_read': 0}})]}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7)\n",
      " Call ID: b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7\n",
      "  Args:\n",
      "    b: 4.0\n",
      "    a: 3.0\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result is 7.\n"
     ]
    }
   ],
   "source": [
    "# Specify a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "\n",
    "# Run\n",
    "messages = react_graph_memory.invoke({\"messages\": messages},config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a8a16-6bf1-48e2-a889-ae04a37c7a2b",
   "metadata": {},
   "source": [
    "If we pass the same `thread_id`, then we can proceed from from the previously logged state checkpoint! \n",
    "\n",
    "In this case, the above conversation is captured in the thread.\n",
    "\n",
    "The `HumanMessage` we pass (`\"Multiply that by 2.\"`) is appended to the above conversation.\n",
    "\n",
    "So, the model now know that `that` refers to the `The sum of 3 and 4 is 7.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee38c6ef-8bfb-4c66-9214-6f474c9b8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here {'messages': [HumanMessage(content='Add 3 and 4.', additional_kwargs={}, response_metadata={}, id='74547d33-921c-46ec-9802-5db5ec4df1b0'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"b\": 4.0, \"a\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--e86ca31b-01ac-4136-8ece-1005b294ca49-0', tool_calls=[{'name': 'add', 'args': {'b': 4.0, 'a': 3.0}, 'id': 'b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 5, 'total_tokens': 137, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='7', name='add', id='eaf6016b-d63d-4b22-b402-e65737fe7679', tool_call_id='b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7'), AIMessage(content='The result is 7.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--d897f114-6051-494a-9703-6b533630e2c2-0', usage_metadata={'input_tokens': 140, 'output_tokens': 7, 'total_tokens': 147, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='vance age is the result of previous execution, what is his age?use history of this run', additional_kwargs={}, response_metadata={}, id='c10d0a3c-0f74-4159-b680-a0e372d9819b'), AIMessage(content=\"I am sorry, I cannot fulfill this request. The previous execution did not involve information about a person's age.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--9fd0bd18-6f98-4cec-bd6f-d86dc893b612-0', usage_metadata={'input_tokens': 165, 'output_tokens': 25, 'total_tokens': 190, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='vance age is the answer of previous execution, what is his age?use history of this run', additional_kwargs={}, response_metadata={}, id='7dc20c0c-55c7-4271-8531-4deab9378c8f'), AIMessage(content=\"I am sorry, I cannot fulfill this request. The previous execution did not involve information about a person's age.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--5348269b-69f9-4ea4-8a63-82631fb0952d-0', usage_metadata={'input_tokens': 208, 'output_tokens': 25, 'total_tokens': 233, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='vance age is the answer of previous executions, look into the data, what is his age?use history of this run', additional_kwargs={}, response_metadata={}, id='4317d3a3-fa0f-4cdb-82a9-04ffa1661352')]}\n",
      "state modifed {'messages': [AIMessage(content=\"I am sorry, I cannot fulfill this request. The previous execution did not involve information about a person's age.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--1553d36b-211f-4b52-afbe-203fa7bc1c01-0', usage_metadata={'input_tokens': 256, 'output_tokens': 25, 'total_tokens': 281, 'input_token_details': {'cache_read': 0}})]}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7)\n",
      " Call ID: b984ce51-fa8c-4ad5-ad0f-c5f8a344efb7\n",
      "  Args:\n",
      "    b: 4.0\n",
      "    a: 3.0\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result is 7.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "vance age is the result of previous execution, what is his age?use history of this run\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am sorry, I cannot fulfill this request. The previous execution did not involve information about a person's age.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "vance age is the answer of previous execution, what is his age?use history of this run\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am sorry, I cannot fulfill this request. The previous execution did not involve information about a person's age.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "vance age is the answer of previous executions, look into the data, what is his age?use history of this run\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am sorry, I cannot fulfill this request. The previous execution did not involve information about a person's age.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"vance age is the answer of previous executions, look into the data, what is his age?use history of this run\")]\n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7774e-566f-4c92-9429-ed953bcacaa5",
   "metadata": {},
   "source": [
    "## LangGraph Studio\n",
    "\n",
    "\n",
    "** DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `module-1/studio/` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72986c-ff6f-4f81-b585-d268e2710e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
